# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wNryFKT-Wq726qia3_wi0JezIuGJ5tQn
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

"""Q1a. summary statistic"""

import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/ingredient.csv')
data.describe()

"""Q1a. parametric and non-parametric test"""

from scipy.stats import ttest_ind, f_oneway, ranksums
t_test_results = ttest_ind(data['a'], data['b'])
print(t_test_results)
wilcoxon_results = ranksums(data['a'], data['b'])
print(wilcoxon_results)

"""Q1a. Correlation matrix"""

import seaborn as sns
import matplotlib.pyplot as plt
correlation_matrix = data.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

"""Q1a. ANOVA"""

anova_results = f_oneway(data['a'], data['b'], data['c'])
print(anova_results)

"""Q1-b. Graphical Analysis"""

# Distribution Study (Histograms)
data.hist(bins=20, figsize=(15, 10))
plt.show()

# Pairwise Scatter Plots
sns.pairplot(data)
plt.show()

# Boxplots
sns.boxplot(data=data, orient='h')
plt.show()

"""Q1c. Cluster Test"""

import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# K-means clustering
k_values = range(2, 10)
silhouette_scores = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    silhouette_scores.append(silhouette_score(scaled_data, kmeans.labels_))

# Plot the Elbow Curve
plt.plot(k_values, silhouette_scores, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.show()

# Choose the optimal k and perform clustering
optimal_k = k_values[np.argmax(silhouette_scores)]
kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans_optimal.fit_predict(scaled_data)

# Visualize Clusters
data['Cluster'] = clusters
sns.pairplot(data, hue='Cluster')
plt.show()

"""Q2. descriptive statistic"""

import pandas as pd
palm_data = pd.read_csv('/content/drive/MyDrive/palm_ffb.csv')
palm_data.describe()

"""Q2. Corelation Statistics"""

# Correlation matrix
correlation_matrix = palm_data.corr()

# Display correlation matrix
print(correlation_matrix)

"""Q2. Time Series Analysis"""

import matplotlib.pyplot as plt

# Set the Date column as the index
palm_data.set_index('Date', inplace=True)

# Plot time series for FFB Yield
plt.figure(figsize=(14, 7))
plt.plot(palm_data['FFB_Yield'], label='FFB Yield')
plt.title('Time Series of FFB Yield')
plt.xlabel('Date')
plt.ylabel('FFB Yield')
plt.legend()
plt.show()

"""Q2. Scatter Plot"""

# Scatter plot between FFB Yield and other variables
plt.figure(figsize=(16, 16))
plt.subplot(3, 2, 1)
plt.scatter(palm_data['SoilMoisture'], palm_data['FFB_Yield'])
plt.title('Soil Moisture vs. FFB Yield')

plt.subplot(3, 2, 2)
plt.scatter(palm_data['Average_Temp'], palm_data['FFB_Yield'])
plt.title('Average Temperature vs. FFB Yield')

# Repeat for other variables...

plt.tight_layout()
plt.show()

"""Q2. Multiple Regression Analysis"""

import statsmodels.api as sm

# Assuming X contains the independent variables and y contains the dependent variable (FFB Yield)
X = palm_data[['SoilMoisture', 'Average_Temp', 'Precipitation', 'Working_days', 'HA_Harvested']]
X = sm.add_constant(X)  # add a constant term to the predictor

y = palm_data['FFB_Yield']

# Fit the model
model = sm.OLS(y, X).fit()

# Display regression summary
print(model.summary())

"""Q3a. Probability of the word “data” occurring in each line:"""

paragraph = """ As a term, data analytics predominantly refers to an assortment of applications, from basic business
intelligence (BI), reporting and online analytical processing (OLAP) to various forms of advanced
analytics. In that sense, it's similar in nature to business analytics, another umbrella term for
approaches to analyzing data -- with the difference that the latter is oriented to business uses, while
data analytics has a broader focus. The expansive view of the term isn't universal, though: In some
cases, people use data analytics specifically to mean advanced analytics, treating BI as a separate
category. Data analytics initiatives can help businesses increase revenues, improve operational
efficiency, optimize marketing campaigns and customer service efforts, respond more quickly to
emerging market trends and gain a competitive edge over rivals -- all with the ultimate goal of
boosting business performance. Depending on the particular application, the data that's analyzed
can consist of either historical records or new information that has been processed for real-time
analytics uses. In addition, it can come from a mix of internal systems and external data sources. At
a high level, data analytics methodologies include exploratory data analysis (EDA), which aims to find
patterns and relationships in data, and confirmatory data analysis (CDA), which applies statistical
techniques to determine whether hypotheses about a data set are true or false. EDA is often
compared to detective work, while CDA is akin to the work of a judge or jury during a court trial -- a
distinction first drawn by statistician John W. Tukey in his 1977 book Exploratory Data Analysis. Data
analytics can also be separated into quantitative data analysis and qualitative data analysis. The
former involves analysis of numerical data with quantifiable variables that can be compared or
measured statistically. The qualitative approach is more interpretive -- it focuses on understanding
the content of non-numerical data like text, images, audio and video, including common phrases,
themes and points of view.
"""
import nltk
nltk.download('punkt')

# Tokenize the paragraph into sentences
sentences = nltk.sent_tokenize(paragraph)

# Calculate the probability for each line
for i, sentence in enumerate(sentences):
    words = nltk.word_tokenize(sentence)
    data_count = words.count("data")
    total_words = len(words)
    probability = data_count / total_words
    print(f"Line {i + 1}: Probability of 'data' = {probability:.4f}")

"""Q3b. Distribution of distinct word counts across all lines:"""

from collections import Counter

# Tokenize the entire paragraph into words
words = nltk.word_tokenize(paragraph)

# Count distinct word occurrences
word_counts = Counter(words)

# Count distribution of distinct word counts
distinct_counts = Counter(word_counts.values())

# Print the distribution
print("Distinct Word Counts Distribution:")
for count, frequency in distinct_counts.items():
    print(f"{count} words: {frequency} lines")

"""Q3c. Probability of the word “analytics” occurring after the word “data”:"""

# Find instances of "data" and "analytics" in the paragraph
data_indices = [i for i, word in enumerate(words) if word.lower() == "data"]
analytics_after_data_count = sum(1 for i in data_indices if i < len(words) - 1 and words[i + 1].lower() == "analytics")

# Calculate the probability
if len(data_indices) > 0:
    probability_analytics_after_data = analytics_after_data_count / len(data_indices)
    print(f"Probability of 'analytics' after 'data': {probability_analytics_after_data:.4f}")
else:
    print("No occurrences of 'data' found in the paragraph.")